"""
Evaluation protocols for transport neural operators.

1. OmegaTransferProtocol:    evaluate at unseen direction counts (Nω sweep)
                              alias: SNTransferProtocol (backward-compatible)
2. ResolutionTransferProtocol: evaluate at unseen spatial resolutions (x2, x4 grids)
3. RegimeSweepProtocol:      evaluate across epsilon values (diffusive to transport)

All protocols load pre-generated data strictly from disk (via data_dir).
If the required split file is missing a RuntimeError is raised with the
exact generate_dataset.py command to run.  No on-the-fly solver calls are
made during evaluation.
"""

from __future__ import annotations
import csv
import logging
import time
from typing import Any, Optional, List, Dict

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader

from ..data.dataset import TransportDataset, collate_fn, resample_omega_directions
from ..data.schema import TransportSample
from .metrics import compute_metrics, aggregate_metrics

logger = logging.getLogger(__name__)


# Regime-sweep epsilon values — must stay in sync with generate_dataset.py.
REGIME_SWEEP_EPSILONS = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0]

# ── shared helper ─────────────────────────────────────────────────────────────

def _load_split_from_disk(
    data_dir: Optional[str],
    benchmark_name: str,
    split: str,
    n_samples: int,
) -> Optional[List[TransportSample]]:
    """
    Try to load a pre-generated split from disk.  Returns None if not found.
    Reads at most n_samples entries.
    """
    if data_dir is None:
        return None
    from pathlib import Path as _Path
    base = _Path(data_dir)
    for suffix in (f"{benchmark_name}_{split}.zarr", f"{benchmark_name}_{split}.zarr.h5"):
        p = base / suffix
        if p.exists():
            from ..data.dataset import TransportDataset as _DS
            ds = _DS(source=p)
            n = min(n_samples, len(ds))
            samples = [ds.reader.read(i) for i in range(n)]
            logger.info(
                f"  [{benchmark_name}/{split}] Loaded {n} samples from disk: {p}"
            )
            return samples
    return None


def _load_split_required(
    data_dir: Optional[str],
    benchmark_name: str,
    split: str,
    n_samples: int,
) -> List[TransportSample]:
    """
    Load a pre-generated evaluation split from disk.  Raises RuntimeError if
    the file is missing — evaluation never runs a solver on-the-fly.

    All evaluation splits must be generated by:
      python scripts/generate_dataset.py --benchmark <bm> --split all_eval
    """
    samples = _load_split_from_disk(data_dir, benchmark_name, split, n_samples)
    if samples is not None:
        return samples

    data_dir_str = data_dir or "<output_dir>"
    raise RuntimeError(
        f"Evaluation split '{split}' not found for benchmark '{benchmark_name}' "
        f"in '{data_dir_str}'.\n"
        f"Pre-generate all evaluation splits before running eval.py:\n"
        f"  python scripts/generate_dataset.py "
        f"--benchmark {benchmark_name} --split all_eval --n_samples <N> "
        f"--output_dir {data_dir_str}"
    )


# ── Test Set ──────────────────────────────────────────────────────────────────

class TestSetProtocol:
    """
    Baseline test-set evaluation protocol.

    Runs the model on the pre-generated test split at training resolution and
    reports aggregate metrics: I_rel_l2, phi_rel_l2, J_rel_l2.

    This is the fundamental "how well does the model do on held-out data at
    training resolution?" metric — the baseline before any transfer test.

    Loads test samples from the provided list (already loaded from disk by eval.py).
    Reports: I_rel_l2, phi_rel_l2, J_rel_l2, runtime_s
    """

    def __init__(
        self,
        model: nn.Module,
        test_samples: List[TransportSample],
        device: Optional[str] = None,
        batch_size: int = 4,
    ):
        self.model = model
        self.test_samples = test_samples
        self.batch_size = batch_size

        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = torch.device(device)

    def run(self) -> Dict[str, Any]:
        """
        Evaluate on test set.  Returns a dict with a single key 'test'
        containing aggregate metrics including J_rel_l2.
        """
        self.model.eval()
        ds = TransportDataset(self.test_samples)
        loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False,
                            collate_fn=collate_fn)

        metric_list = []
        t0 = time.time()
        with torch.no_grad():
            for batch in loader:
                batch_dev = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                             for k, v in batch.items()}
                pred = self.model(batch_dev)
                m = compute_metrics(pred, batch_dev, self.device)
                metric_list.append(m)
        t1 = time.time()

        summary = aggregate_metrics(metric_list)
        summary["n_samples"] = len(self.test_samples)
        summary["runtime_s"] = t1 - t0
        logger.info(
            f"Test set: I_rel_l2={summary.get('I_rel_l2', float('nan')):.4e}  "
            f"phi_rel_l2={summary.get('phi_rel_l2', float('nan')):.4e}  "
            f"J_rel_l2={summary.get('J_rel_l2', float('nan')):.4e}"
        )
        return {"test": summary}

    def to_csv(self, results: Dict[str, Any], path: str):
        rows = list(results.values())
        if not rows:
            return
        fieldnames = sorted(rows[0].keys())
        with open(path, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(rows)


# ── Omega Transfer (direction-count / Nω sweep) ───────────────────────────────

class OmegaTransferProtocol:
    """
    Direction-count (Nω) transfer evaluation protocol.

    Tests discretization-agnosticism: model trained on Nω directions, evaluated
    at unseen direction counts.  Uses a 2D uniform-azimuth quadrature with Nω
    equally spaced angles and equal weights; transfer means changing Nω at eval.

    Uses the provided test_samples (real data if available).
    Reports: I_rel_l2, phi_rel_l2, J_rel_l2 vs n_omega
    """

    def __init__(
        self,
        model: nn.Module,
        test_samples: List[TransportSample],
        train_n_omega: int = 8,
        test_n_omegas: List[int] = None,
        device: Optional[str] = None,
        batch_size: int = 4,
    ):
        self.model = model
        self.test_samples = test_samples
        self.train_n_omega = train_n_omega
        self.test_n_omegas = test_n_omegas or [4, 8, 16, 32, 64]
        self.batch_size = batch_size

        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = torch.device(device)

    def run(self) -> Dict[str, Any]:
        """
        Run omega (direction-count) transfer evaluation.

        Returns:
            dict mapping n_omega -> aggregate metrics
        """
        self.model.eval()
        results = {}

        for n_omega in self.test_n_omegas:
            logger.info(f"Omega transfer: evaluating at Nω={n_omega}")
            # Resample directions — one RNG shared across all samples so each
            # sample gets distinct angular directions.
            rng = np.random.default_rng(42)
            resampled = [
                resample_omega_directions(s, n_omega, rng)
                for s in self.test_samples
            ]

            ds = TransportDataset(resampled)
            loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False,
                                collate_fn=collate_fn)

            metric_list = []
            t0 = time.time()
            with torch.no_grad():
                for batch in loader:
                    batch_dev = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                                 for k, v in batch.items()}
                    pred = self.model(batch_dev)
                    m = compute_metrics(pred, batch_dev, self.device)
                    m.n_omega = n_omega
                    metric_list.append(m)
            t1 = time.time()

            summary = aggregate_metrics(metric_list)
            summary["n_omega"] = n_omega
            summary["runtime_s"] = t1 - t0
            summary["is_train_nw"] = (n_omega == self.train_n_omega)
            results[n_omega] = summary
            logger.info(f"  Nω={n_omega}: I_rel_l2={summary.get('I_rel_l2', float('nan')):.4e}")

        return results

    def to_csv(self, results: Dict[str, Any], path: str):
        rows = list(results.values())
        if not rows:
            return
        fieldnames = sorted(rows[0].keys())
        with open(path, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(rows)


# Backward-compatible alias
SNTransferProtocol = OmegaTransferProtocol


# ── Resolution Transfer ───────────────────────────────────────────────────────

class ResolutionTransferProtocol:
    """
    Resolution Transfer Evaluation Protocol.

    Trains on coarse spatial grid, evaluates at higher resolutions.
    The model handles this naturally since x is a continuous query.

    At base resolution (x1):  uses the provided real test samples.
    At higher resolutions:    loads from pre-generated resolution_xN splits on disk.
                              Raises RuntimeError if splits are missing.

    Reports: I_rel_l2, phi_rel_l2, J_rel_l2 vs spatial_resolution and runtime
    """

    def __init__(
        self,
        model: nn.Module,
        base_spatial_shape: tuple,
        resolution_multipliers: List[int] = None,
        n_groups: int = 1,
        benchmark_name: str = "c5g7",
        n_test_samples: int = 50,
        device: Optional[str] = None,
        batch_size: int = 4,
        n_omega: int = 8,
        base_test_samples: Optional[List[TransportSample]] = None,
        data_dir: Optional[str] = None,
    ):
        self.model = model
        self.base_shape = base_spatial_shape
        self.resolution_multipliers = resolution_multipliers or [1, 2, 4]
        self.n_groups = n_groups
        self.benchmark_name = benchmark_name
        self.n_test = n_test_samples
        self.batch_size = batch_size
        self.n_omega = n_omega
        self.base_test_samples = base_test_samples
        self.data_dir = data_dir

        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = torch.device(device)

    def _samples_at_resolution(self, multiplier: int) -> List[TransportSample]:
        """Load the pre-generated resolution_xN split from disk."""
        split = f"resolution_x{multiplier}"
        return _load_split_required(self.data_dir, self.benchmark_name, split, self.n_test)

    def run(self) -> Dict[str, Any]:
        self.model.eval()
        results = {}

        for mult in self.resolution_multipliers:
            shape = tuple(s * mult for s in self.base_shape)
            logger.info(f"Resolution transfer: evaluating at shape={shape} (x{mult})")

            if mult == 1:
                if self.base_test_samples is None:
                    raise RuntimeError(
                        "ResolutionTransferProtocol: base_test_samples must be provided "
                        "for multiplier=1.  Pass base_test_samples=test_samples when "
                        "constructing the protocol."
                    )
                samples = self.base_test_samples
                logger.info(f"  (using {len(samples)} pre-generated test samples at x1)")
            else:
                samples = self._samples_at_resolution(mult)

            ds = TransportDataset(samples)
            loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False,
                                collate_fn=collate_fn)

            metric_list = []
            t0 = time.time()
            with torch.no_grad():
                for batch in loader:
                    batch_dev = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                                 for k, v in batch.items()}
                    try:
                        pred = self.model(batch_dev)
                        m = compute_metrics(pred, batch_dev, self.device)
                        m.n_spatial = int(np.prod(shape))
                        metric_list.append(m)
                    except Exception as e:
                        logger.warning(f"Forward failed at resolution x{mult}: {e}")
            t1 = time.time()

            summary = aggregate_metrics(metric_list)
            summary["resolution_multiplier"] = mult
            summary["spatial_shape"] = str(shape)
            summary["n_spatial"] = int(np.prod(shape))
            summary["runtime_s"] = t1 - t0
            results[mult] = summary
            logger.info(f"  x{mult}: I_rel_l2={summary.get('I_rel_l2', float('nan')):.4e}, runtime={t1-t0:.2f}s")

        return results

    def to_csv(self, results: Dict[str, Any], path: str):
        rows = list(results.values())
        if not rows:
            return
        fieldnames = sorted(rows[0].keys())
        with open(path, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(rows)


# ── Regime Sweep ──────────────────────────────────────────────────────────────

class RegimeSweepProtocol:
    """
    Regime Sweep Evaluation Protocol.

    Evaluates model across epsilon values spanning transport to diffusion limits.
    Loads from pre-generated regime_eps<value> splits on disk.
    Raises RuntimeError if splits are missing.

    Reports I_rel_l2, phi_rel_l2, J_rel_l2 vs epsilon.
    """

    def __init__(
        self,
        model: nn.Module,
        epsilon_values: List[float] = None,
        spatial_shape: tuple = (16, 16),
        n_omega: int = 8,
        n_groups: int = 1,
        benchmark_name: str = "c5g7",
        n_test_samples: int = 50,
        device: Optional[str] = None,
        batch_size: int = 4,
        data_dir: Optional[str] = None,
    ):
        self.model = model
        self.epsilon_values = epsilon_values or REGIME_SWEEP_EPSILONS
        self.spatial_shape = spatial_shape
        self.n_omega = n_omega
        self.n_groups = n_groups
        self.benchmark_name = benchmark_name
        self.n_test = n_test_samples
        self.batch_size = batch_size
        self.data_dir = data_dir

        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = torch.device(device)

    def run(self) -> Dict[str, Any]:
        self.model.eval()
        results = {}

        for eps in self.epsilon_values:
            logger.info(f"Regime sweep: evaluating at epsilon={eps:.4g}")
            eps_tag = f"{eps:.3f}".rstrip("0").rstrip(".")
            split = f"regime_eps{eps_tag}"
            samples = _load_split_required(
                self.data_dir, self.benchmark_name, split, self.n_test
            )

            ds = TransportDataset(samples)
            loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False,
                                collate_fn=collate_fn)

            metric_list = []
            with torch.no_grad():
                for batch in loader:
                    batch_dev = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v
                                 for k, v in batch.items()}
                    try:
                        pred = self.model(batch_dev)
                        m = compute_metrics(pred, batch_dev, self.device)
                        m.epsilon = eps
                        metric_list.append(m)
                    except Exception as e:
                        logger.warning(f"Forward failed at epsilon={eps}: {e}")

            summary = aggregate_metrics(metric_list)
            summary["epsilon"] = eps
            summary["log_epsilon"] = float(np.log10(eps))
            results[eps] = summary
            logger.info(f"  eps={eps:.4g}: I_rel_l2={summary.get('I_rel_l2', float('nan')):.4e}, "
                        f"phi_rel_l2={summary.get('phi_rel_l2', float('nan')):.4e}")

        return results

    def to_csv(self, results: Dict[str, Any], path: str):
        rows = list(results.values())
        if not rows:
            return
        fieldnames = sorted(rows[0].keys())
        with open(path, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(rows)
